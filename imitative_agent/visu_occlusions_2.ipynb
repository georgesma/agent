{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1f0aae-0ad2-4a21-b363-e92dc7a1c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from imitative_agent import ImitativeAgent\n",
    "from lib.dataset_wrapper import Dataset\n",
    "from lib import utils\n",
    "from lib import abx_utils\n",
    "from lib import notebooks\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f5e19cb-778d-4ddb-b31d-d67db98a1bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/vpaul/Documents/Inner_Speech/agent/imitative_agent\n",
      "../out/imitative_agent/bfa2f1a4bdf7d85496bc8c867342f96e-0/\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to() received an invalid combination of arguments - got (tuple), but expected one of:\n * (torch.device device, torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (Tensor tensor, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(agent_path)\n\u001b[0;32m---> 10\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mImitativeAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_nn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjerk_loss_ceil\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.014\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/vpaul/Documents/Inner_Speech/agent/imitative_agent/imitative_agent.py:116\u001b[0m, in \u001b[0;36mImitativeAgent.reload\u001b[0;34m(save_path, load_nn)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    115\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[0;32m--> 116\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mImitativeAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/sound_scaler.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    119\u001b[0m     agent\u001b[38;5;241m.\u001b[39msound_scaler \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m/mnt/c/Users/vpaul/Documents/Inner_Speech/agent/imitative_agent/imitative_agent.py:33\u001b[0m, in \u001b[0;36mImitativeAgent.__init__\u001b[0;34m(self, config, load_nn)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_nn:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynthesizer \u001b[38;5;241m=\u001b[39m Synthesizer\u001b[38;5;241m.\u001b[39mreload(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (SYNTHESIZERS_PATH, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthesizer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     32\u001b[0m     )\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/mnt/c/Users/vpaul/Documents/Inner_Speech/agent/imitative_agent/imitative_agent.py:58\u001b[0m, in \u001b[0;36mImitativeAgent._build_nn\u001b[0;34m(self, model_config)\u001b[0m\n\u001b[1;32m     40\u001b[0m inverse_model \u001b[38;5;241m=\u001b[39m SimpleLSTM(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msound_dim,\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mart_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minverse_model\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m direct_model \u001b[38;5;241m=\u001b[39m FeedForward(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mart_dim,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msound_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect_model\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn \u001b[38;5;241m=\u001b[39m \u001b[43mImitativeAgentNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverse_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirect_model\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1141\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves and/or casts the parameters and buffers.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    This can be called as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1141\u001b[0m     device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (dtype\u001b[38;5;241m.\u001b[39mis_floating_point \u001b[38;5;129;01mor\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_complex):\n",
      "\u001b[0;31mTypeError\u001b[0m: to() received an invalid combination of arguments - got (tuple), but expected one of:\n * (torch.device device, torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (torch.dtype dtype, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n * (Tensor tensor, bool non_blocking, bool copy, *, torch.memory_format memory_format)\n"
     ]
    }
   ],
   "source": [
    "agents_path = glob(\"../out/imitative_agent/*/\")\n",
    "agents_path.sort()\n",
    "\n",
    "agents_alias = {}\n",
    "agents_group = {}\n",
    "\n",
    "for agent_path in agents_path:\n",
    "    print(os.getcwd())\n",
    "    print(agent_path)\n",
    "    agent = ImitativeAgent.reload(agent_path, load_nn=False)\n",
    "    config = agent.config\n",
    "    \n",
    "    #if config[\"training\"][\"jerk_loss_ceil\"] != 0.014: continue\n",
    "        \n",
    "    agent_i = agent_path[-2]\n",
    "    agent_alias = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\",\n",
    "        f\"jerk_c={config['training']['jerk_loss_ceil']}\",\n",
    "        f\"jerk_w={config['training']['jerk_loss_weight']}\",\n",
    "        f\"bi={config['model']['inverse_model']['bidirectional']}\",\n",
    "        f\"({agent_i})\",\n",
    "    ))\n",
    "    agents_alias[agent_alias] = agent_path\n",
    "    \n",
    "    agent_group = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\",\n",
    "        f\"jerk_c={config['training']['jerk_loss_ceil']}\",\n",
    "        f\"jerk_w={config['training']['jerk_loss_weight']}\",\n",
    "        f\"bi={config['model']['inverse_model']['bidirectional']}\",\n",
    "    ))\n",
    "    if agent_group not in agents_group:\n",
    "        agents_group[agent_group] = []\n",
    "    agents_group[agent_group].append(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b907ea-5f85-4da3-9697-c4944563532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TONGUE_CONSONANTS = [\"p\", \"b\", \"t\", \"d\", \"k\", \"g\"]\n",
    "DETECTION_METHODS = {\n",
    "    \"p\": \"lips\",\n",
    "    \"b\": \"lips\",\n",
    "    \"t\": \"tongue_tip\",\n",
    "    \"d\": \"tongue_tip\",\n",
    "    \"k\": \"tongue_mid\",\n",
    "    \"g\": \"tongue_mid\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c5fb8b-4cbf-47d1-bc6d-485a00b2edfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fec29027ab448393cb80659d9811ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agents_ema = {}\n",
    "datasets_occlusions = {}\n",
    "\n",
    "for agent_alias, agent_path in tqdm(agents_alias.items()):\n",
    "    agent_ema = agents_ema[agent_path] = {}\n",
    "    \n",
    "    agent = ImitativeAgent.reload(agent_path)\n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    \n",
    "    main_dataset = agent.get_main_dataset()\n",
    "    agent_features = agent.repeat_datasplit(None)\n",
    "    \n",
    "    for dataset_name, dataset_features in agent_features.items():\n",
    "        if dataset_name not in datasets_occlusions:\n",
    "            dataset = Dataset(dataset_name)\n",
    "            palate = dataset.palate\n",
    "            vowels = dataset.phones_infos[\"vowels\"]\n",
    "            datasets_lab = {dataset_name: dataset.lab}\n",
    "            datasets_ema = {dataset_name: dataset.get_items_data(\"ema\")}\n",
    "            consonants_indexes = abx_utils.get_datasets_phones_indexes(\n",
    "                datasets_lab, TONGUE_CONSONANTS, vowels\n",
    "            )\n",
    "            datasets_occlusions[dataset_name] = abx_utils.get_occlusions_indexes(\n",
    "                TONGUE_CONSONANTS, consonants_indexes, DETECTION_METHODS, datasets_ema, palate,\n",
    "            )\n",
    "        \n",
    "        items_estimated_ema = agent_ema[dataset_name] = {}\n",
    "        \n",
    "        items_estimated_art = dataset_features[\"art_estimated\"]\n",
    "        for item_name, item_estimated_art in items_estimated_art.items():\n",
    "            item_estimated_ema = synth_dataset.art_to_ema(item_estimated_art)\n",
    "            items_estimated_ema[item_name] = item_estimated_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286e74fb-89f5-4446-9a44-70ab93fe95dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d26cbf2e6c0417d8386e9b7b28d6220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset_name', options=(), value=None), Output()), _dom_classes=('…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_dataset(dataset_name):\n",
    "    dataset = Dataset(dataset_name)\n",
    "    items_ema = dataset.get_items_data(\"ema\")\n",
    "    dataset_occlusions = datasets_occlusions[dataset_name]\n",
    "    palate = dataset.palate\n",
    "    \n",
    "    display_xlim = (dataset.ema_limits[\"xmin\"] * 0.95, dataset.ema_limits[\"xmax\"] * 1.05)\n",
    "    display_ylim = (dataset.ema_limits[\"ymin\"] * 0.95, dataset.ema_limits[\"ymax\"] * 1.05)\n",
    "    \n",
    "    def show_occlusions(offset=2):\n",
    "        consonants_stats = {}\n",
    "        for consonant, occlusions in dataset_occlusions.items():\n",
    "            plt.figure(figsize=(12, 3), dpi=60)\n",
    "\n",
    "            ax_start = plt.subplot(121, aspect=\"equal\")\n",
    "            ax_start.set_title(\"%s start (PB original)\" % consonant)\n",
    "            ax_start.set_xlim(*display_xlim)\n",
    "            ax_start.set_ylim(*display_ylim)\n",
    "            ax_start.plot(palate[:, 0], palate[:, 1])\n",
    "            ax_start.set_xticks([])\n",
    "            ax_start.set_yticks([])\n",
    "\n",
    "            ax_stop = plt.subplot(122, aspect=\"equal\")\n",
    "            ax_stop.set_title(\"%s stop (PB original)\" % consonant)\n",
    "            ax_stop.set_xlim(*display_xlim)\n",
    "            ax_stop.set_ylim(*display_ylim)\n",
    "            ax_stop.plot(palate[:, 0], palate[:, 1])\n",
    "            ax_stop.set_xticks([])\n",
    "            ax_stop.set_yticks([])\n",
    "\n",
    "            occlusions_start_ema = []\n",
    "            occlusions_stop_ema = []\n",
    "            for occlusion in occlusions:\n",
    "                item_ema = items_ema[occlusion[1]]\n",
    "                occlusions_start_ema.append(item_ema[occlusion[2] - offset])\n",
    "                occlusions_stop_ema.append(item_ema[occlusion[3] + offset])\n",
    "            occlusions_start_ema = np.array(occlusions_start_ema)\n",
    "            occlusions_stop_ema = np.array(occlusions_stop_ema) \n",
    "            \n",
    "            occlusions_stats = consonants_stats[consonant] = {}\n",
    "            for occlusions_type, occlusions_ema in {\"start\": occlusions_start_ema, \"stop\": occlusions_stop_ema}.items():\n",
    "                lips_distance = np.sqrt(np.sum((occlusions_ema[:, 10:12] - occlusions_ema[:, 8:10]) ** 2, axis=1))\n",
    "                occlusions_stats[\"%s_lips\" % occlusions_type] = \"%.2f ±%.2f\" % (lips_distance.mean(), lips_distance.std())\n",
    "                \n",
    "                tongue_tip_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 2:4], palate)\n",
    "                occlusions_stats[\"%s_tongue_tip\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_tip_distance.mean(), tongue_tip_distance.std())\n",
    "                \n",
    "                tongue_mid_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 4:6], palate)\n",
    "                occlusions_stats[\"%s_tongue_mid\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_mid_distance.mean(), tongue_mid_distance.std())\n",
    "\n",
    "            ax_start.scatter(occlusions_start_ema[:, 0::2], occlusions_start_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "            ax_stop.scatter(occlusions_stop_ema[:, 0::2], occlusions_stop_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "\n",
    "            plt.subplots_adjust(wspace=-.1)\n",
    "            plt.show()\n",
    "            \n",
    "        consonants_stats = pd.DataFrame.from_dict(consonants_stats, orient=\"index\")\n",
    "        display(consonants_stats)\n",
    "    \n",
    "    ipw.interact(show_occlusions, offset=(0, 10))\n",
    "\n",
    "ipw.interactive(show_dataset, dataset_name=datasets_occlusions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22e4695-2f9d-4c6e-a234-bb55a677660e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c89cd2e18a472cb16e034cac74e407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='agent_alias', options=(), value=None), Output()), _dom_classes=('w…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_agent(agent_alias):\n",
    "    agent_path = agents_alias[agent_alias]\n",
    "    agent = ImitativeAgent.reload(agent_path, load_nn=False)\n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    palate = synth_dataset.palate\n",
    "    agent_ema = agents_ema[agent_path]\n",
    "    \n",
    "    display_xlim = (synth_dataset.ema_limits[\"xmin\"] * 0.95, synth_dataset.ema_limits[\"xmax\"] * 1.05)\n",
    "    display_ylim = (synth_dataset.ema_limits[\"ymin\"] * 0.95, synth_dataset.ema_limits[\"ymax\"] * 1.05)\n",
    "    \n",
    "    def show_occlusions(offset=2):\n",
    "        consonants_stats = {}\n",
    "    \n",
    "        for dataset_name in agent.config[\"dataset\"][\"names\"]:\n",
    "            dataset = Dataset(dataset_name)\n",
    "            items_ema = agent_ema[dataset_name]\n",
    "            dataset_occlusions = datasets_occlusions[dataset_name]\n",
    "\n",
    "            for consonant, occlusions in dataset_occlusions.items():\n",
    "                plt.figure(figsize=(12, 3), dpi=60)\n",
    "\n",
    "                ax_start = plt.subplot(121, aspect=\"equal\")\n",
    "                ax_start.set_title(\"%s start (jerk=%s)\" % (consonant, agent.config[\"training\"][\"jerk_loss_weight\"]))\n",
    "                ax_start.set_xlim(*display_xlim)\n",
    "                ax_start.set_ylim(*display_ylim)\n",
    "                ax_start.plot(palate[:, 0], palate[:, 1])\n",
    "                ax_start.set_xticks([])\n",
    "                ax_start.set_yticks([])\n",
    "\n",
    "                ax_stop = plt.subplot(122, aspect=\"equal\")\n",
    "                ax_stop.set_title(\"%s stop (jerk=%s)\" % (consonant, agent.config[\"training\"][\"jerk_loss_weight\"]))\n",
    "                ax_stop.set_xlim(*display_xlim)\n",
    "                ax_stop.set_ylim(*display_ylim)\n",
    "                ax_stop.plot(palate[:, 0], palate[:, 1])\n",
    "                ax_stop.set_xticks([])\n",
    "                ax_stop.set_yticks([])\n",
    "\n",
    "                occlusions_start_ema = []\n",
    "                occlusions_stop_ema = []\n",
    "                for occlusion in occlusions:\n",
    "                    item_ema = items_ema[occlusion[1]]\n",
    "                    occlusions_start_ema.append(item_ema[occlusion[2] - offset])\n",
    "                    occlusions_stop_ema.append(item_ema[occlusion[3] + offset])\n",
    "\n",
    "                occlusions_start_ema = np.array(occlusions_start_ema)\n",
    "                occlusions_stop_ema = np.array(occlusions_stop_ema) \n",
    "                \n",
    "                occlusions_stats = consonants_stats[consonant] = {}\n",
    "                for occlusions_type, occlusions_ema in {\"start\": occlusions_start_ema, \"stop\": occlusions_stop_ema}.items():\n",
    "                    lips_distance = np.sqrt(np.sum((occlusions_ema[:, 10:12] - occlusions_ema[:, 8:10]) ** 2, axis=1))\n",
    "                    occlusions_stats[\"%s_lips\" % occlusions_type] = \"%.2f ±%.2f\" % (lips_distance.mean(), lips_distance.std())\n",
    "\n",
    "                    tongue_tip_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 2:4], palate)\n",
    "                    occlusions_stats[\"%s_tongue_tip\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_tip_distance.mean(), tongue_tip_distance.std())\n",
    "\n",
    "                    tongue_mid_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 4:6], palate)\n",
    "                    occlusions_stats[\"%s_tongue_mid\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_mid_distance.mean(), tongue_mid_distance.std())\n",
    "\n",
    "                ax_start.scatter(occlusions_start_ema[:, 0::2], occlusions_start_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "                ax_stop.scatter(occlusions_stop_ema[:, 0::2], occlusions_stop_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "\n",
    "                plt.subplots_adjust(wspace=-.1)\n",
    "                plt.show()\n",
    "                 \n",
    "        consonants_stats = pd.DataFrame.from_dict(consonants_stats, orient=\"index\")\n",
    "        display(consonants_stats)\n",
    "    ipw.interact(show_occlusions, offset=(0, 10))\n",
    "\n",
    "ipw.interactive(show_agent, agent_alias=sorted(agents_alias.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47802e7-cd60-4116-8dd5-a21e5af237b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

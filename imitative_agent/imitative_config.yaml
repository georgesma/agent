model:
  direct_model:
    hidden_layers: [256, 256, 256, 256]
    activation: relu
    batch_norm: true
    dropout_p: 0.25
  inverse_model:
    num_layers: 2
    hidden_size: 32
    dropout_p: 0.25
    bidirectional: true
synthesizer:
  name: ea587b76c95fecef01cfd16c7f5f289d-1    #dn=pb2007-nd=2-hl=256,256,256,256-in=ema-out=cepstrum-0
dataset:
  names: [pb2007]
  sound_type: cepstrum
  datasplits_size: [64, 16, 20] # train/validation/test in percentage
  batch_size: 8
  num_workers: 6
  shuffle_between_epochs: true
training:
  learning_rate: 0.001
  max_epochs: 500
  patience: 25
  jerk_loss_weight: 0
  jerk_loss_ceil: 0
